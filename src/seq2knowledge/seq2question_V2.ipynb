{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 512  # Latent dimensionality of the encoding space.\n",
    "# Path to the data txt file on disk.\n",
    "num_samples=100000\n",
    "data_path = 'questions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text,qty = line.split(',')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + \" \"+qty + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 29631\n",
      "Number of unique input tokens: 27\n",
      "Number of unique output tokens: 30\n",
      "Max sequence length for inputs: 33\n",
      "Max sequence length for outputs: 45\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\tensorflow-1.13.1-py3.6-win-amd64.egg\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 23704 samples, validate on 5927 samples\n",
      "Epoch 1/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 0.9106 - acc: 0.7395 - val_loss: 0.2989 - val_acc: 0.8923\n",
      "Epoch 2/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.1974 - acc: 0.9289 - val_loss: 0.1527 - val_acc: 0.9435\n",
      "Epoch 3/100\n",
      "23704/23704 [==============================] - 234s 10ms/step - loss: 0.1513 - acc: 0.9398 - val_loss: 0.1318 - val_acc: 0.9482\n",
      "Epoch 4/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.1223 - acc: 0.9502 - val_loss: 0.1123 - val_acc: 0.9565\n",
      "Epoch 5/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0986 - acc: 0.9605 - val_loss: 0.0777 - val_acc: 0.9690\n",
      "Epoch 6/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0739 - acc: 0.9708 - val_loss: 0.0517 - val_acc: 0.9794\n",
      "Epoch 7/100\n",
      "23704/23704 [==============================] - 234s 10ms/step - loss: 0.0456 - acc: 0.9831 - val_loss: 0.0180 - val_acc: 0.9942\n",
      "Epoch 8/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0216 - acc: 0.9935 - val_loss: 0.0030 - val_acc: 0.9995\n",
      "Epoch 9/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0133 - acc: 0.9964 - val_loss: 8.2288e-04 - val_acc: 0.9999\n",
      "Epoch 10/100\n",
      "23704/23704 [==============================] - 234s 10ms/step - loss: 0.0086 - acc: 0.9977 - val_loss: 3.3858e-04 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 2.7359e-04 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 1.2323e-04 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "23704/23704 [==============================] - 234s 10ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 9.8685e-05 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 5.9090e-05 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 6.7104e-05 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "23704/23704 [==============================] - 234s 10ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0093 - val_acc: 0.9976\n",
      "Epoch 17/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 5.0742e-05 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0040 - val_acc: 0.9987\n",
      "Epoch 19/100\n",
      "23704/23704 [==============================] - 234s 10ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 1.1295e-04 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 8.4104e-04 - acc: 0.9998 - val_loss: 2.0199e-05 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 8.7683e-04 - acc: 0.9998 - val_loss: 3.4549e-06 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 8.4152e-04 - acc: 0.9998 - val_loss: 5.0419e-06 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 7.9660e-04 - acc: 0.9998 - val_loss: 1.3165e-05 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 6.0577e-04 - acc: 0.9999 - val_loss: 1.6087e-05 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 6.9278e-04 - acc: 0.9998 - val_loss: 1.9958e-06 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 4.7904e-04 - acc: 0.9999 - val_loss: 2.0711e-06 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 4.7245e-04 - acc: 0.9999 - val_loss: 8.1180e-05 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 5.1361e-04 - acc: 0.9999 - val_loss: 5.6848e-05 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 3.0781e-04 - acc: 0.9999 - val_loss: 2.4956e-06 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 4.5598e-04 - acc: 0.9999 - val_loss: 8.2367e-06 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 4.7193e-04 - acc: 0.9999 - val_loss: 6.7373e-06 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 1.1187e-05 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 3.0571e-04 - acc: 0.9999 - val_loss: 8.9011e-05 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 2.7693e-04 - acc: 1.0000 - val_loss: 0.0133 - val_acc: 0.9974\n",
      "Epoch 35/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 4.7446e-04 - acc: 0.9999 - val_loss: 8.2802e-07 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 3.1644e-04 - acc: 0.9999 - val_loss: 7.5253e-07 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 3.6758e-04 - acc: 0.9999 - val_loss: 1.0868e-06 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 2.6833e-04 - acc: 0.9999 - val_loss: 2.1908e-07 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 2.3686e-04 - acc: 0.9999 - val_loss: 6.5360e-06 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 2.9955e-04 - acc: 1.0000 - val_loss: 1.1309e-06 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 2.4577e-04 - acc: 1.0000 - val_loss: 1.6122e-07 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "23704/23704 [==============================] - 235s 10ms/step - loss: 5.9175e-05 - acc: 1.0000 - val_loss: 1.4037e-07 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.4446e-07 - acc: 1.0000 - val_loss: 1.2609e-07 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.3119e-07 - acc: 1.0000 - val_loss: 1.2223e-07 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2725e-07 - acc: 1.0000 - val_loss: 1.2102e-07 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2531e-07 - acc: 1.0000 - val_loss: 1.2035e-07 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2412e-07 - acc: 1.0000 - val_loss: 1.1992e-07 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2333e-07 - acc: 1.0000 - val_loss: 1.1974e-07 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2277e-07 - acc: 1.0000 - val_loss: 1.1960e-07 - val_acc: 1.0000\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2234e-07 - acc: 1.0000 - val_loss: 1.1950e-07 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2201e-07 - acc: 1.0000 - val_loss: 1.1942e-07 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2175e-07 - acc: 1.0000 - val_loss: 1.1939e-07 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.2152e-07 - acc: 1.0000 - val_loss: 1.1935e-07 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2133e-07 - acc: 1.0000 - val_loss: 1.1932e-07 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2117e-07 - acc: 1.0000 - val_loss: 1.1929e-07 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2104e-07 - acc: 1.0000 - val_loss: 1.1929e-07 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2092e-07 - acc: 1.0000 - val_loss: 1.1927e-07 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2082e-07 - acc: 1.0000 - val_loss: 1.1927e-07 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2073e-07 - acc: 1.0000 - val_loss: 1.1926e-07 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.2065e-07 - acc: 1.0000 - val_loss: 1.1925e-07 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.2057e-07 - acc: 1.0000 - val_loss: 1.1925e-07 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.2051e-07 - acc: 1.0000 - val_loss: 1.1924e-07 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2045e-07 - acc: 1.0000 - val_loss: 1.1924e-07 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.2039e-07 - acc: 1.0000 - val_loss: 1.1924e-07 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2034e-07 - acc: 1.0000 - val_loss: 1.1924e-07 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2030e-07 - acc: 1.0000 - val_loss: 1.1923e-07 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2025e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2021e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2017e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2014e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2011e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2007e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2005e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.2002e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.2000e-07 - acc: 1.0000 - val_loss: 1.1923e-07 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.1997e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.1995e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.1993e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.1991e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.1989e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.1988e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.1986e-07 - acc: 1.0000 - val_loss: 1.1923e-07 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.1984e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.1983e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "23704/23704 [==============================] - 236s 10ms/step - loss: 1.1981e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "23704/23704 [==============================] - 238s 10ms/step - loss: 1.1980e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "23704/23704 [==============================] - 237s 10ms/step - loss: 1.1978e-07 - acc: 1.0000 - val_loss: 1.1923e-07 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "23704/23704 [==============================] - 238s 10ms/step - loss: 1.1977e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "23704/23704 [==============================] - 238s 10ms/step - loss: 1.1976e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1975e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "23704/23704 [==============================] - 238s 10ms/step - loss: 1.1974e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1973e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1972e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "23704/23704 [==============================] - 238s 10ms/step - loss: 1.1970e-07 - acc: 1.0000 - val_loss: 1.1922e-07 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1969e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1968e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1967e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1966e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1965e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "23704/23704 [==============================] - 239s 10ms/step - loss: 1.1965e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nanoscopy\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history=model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.plot(history.history['acc'],color='red',linewidth=2)\n",
    "plt.plot(history.history['val_acc'],linewidth=2)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "fig=plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig('accuracy_lstm.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,2))\n",
    "plt.plot(history.history['loss'],color='red',linewidth=2)\n",
    "plt.plot(history.history['val_loss'],linewidth=2)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "fig=plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig('loss_lstm.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: there are nine pumpkins\n",
      "Decoded sentence: how many pumpkins are there? nine\n",
      "\n",
      "-\n",
      "Input sentence: there are one mango\n",
      "Decoded sentence: how many mangos are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are four rabbits\n",
      "Decoded sentence: how many rabbits are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are three oranges\n",
      "Decoded sentence: how many oranges are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are two icecreams\n",
      "Decoded sentence: how many icecreams are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are five eapots\n",
      "Decoded sentence: how many eapots are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are one cow\n",
      "Decoded sentence: how many cows are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are nine cupcakes\n",
      "Decoded sentence: how many cupcakes are there? nine\n",
      "\n",
      "-\n",
      "Input sentence: there are three umbrellas\n",
      "Decoded sentence: how many umbrellas are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are four balloons\n",
      "Decoded sentence: how many balloons are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are four ducks\n",
      "Decoded sentence: how many ducks are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are eight ants\n",
      "Decoded sentence: how many ants are there? eight\n",
      "\n",
      "-\n",
      "Input sentence: there are four ants\n",
      "Decoded sentence: how many ants are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are two butterflys\n",
      "Decoded sentence: how many butterflys are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are six tops\n",
      "Decoded sentence: how many tops are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are two ants\n",
      "Decoded sentence: how many ants are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are six elephants\n",
      "Decoded sentence: how many elephants are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are five trees\n",
      "Decoded sentence: how many trees are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are one ladybug\n",
      "Decoded sentence: how many ladybugs are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are three pumpkins\n",
      "Decoded sentence: how many pumpkins are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are four sharks\n",
      "Decoded sentence: how many sharks are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are two balls\n",
      "Decoded sentence: how many balls are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are eight apples\n",
      "Decoded sentence: how many apples are there? eight\n",
      "\n",
      "-\n",
      "Input sentence: there are one ball\n",
      "Decoded sentence: how many balls are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two suns\n",
      "Decoded sentence: how many suns are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are two balloons\n",
      "Decoded sentence: how many balloons are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are two suns\n",
      "Decoded sentence: how many suns are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are three carrots\n",
      "Decoded sentence: how many carrots are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are one lollipop\n",
      "Decoded sentence: how many lollipops are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are five sharks\n",
      "Decoded sentence: how many sharks are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are eight lollipops\n",
      "Decoded sentence: how many lollipops are there? eight\n",
      "\n",
      "-\n",
      "Input sentence: there are one sun\n",
      "Decoded sentence: how many suns are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are four pumpkins\n",
      "Decoded sentence: how many pumpkins are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are four suns\n",
      "Decoded sentence: how many suns are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are one apple\n",
      "Decoded sentence: how many apples are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two frogs\n",
      "Decoded sentence: how many frogs are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are three fishs\n",
      "Decoded sentence: how many fishs are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are eight butterflys\n",
      "Decoded sentence: how many butterflys are there? eight\n",
      "\n",
      "-\n",
      "Input sentence: there are seven lollipops\n",
      "Decoded sentence: how many lollipops are there? seven\n",
      "\n",
      "-\n",
      "Input sentence: there are one spider\n",
      "Decoded sentence: how many spiders are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two fishs\n",
      "Decoded sentence: how many fishs are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are two balloons\n",
      "Decoded sentence: how many balloons are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are two cups\n",
      "Decoded sentence: how many cups are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are two icecreams\n",
      "Decoded sentence: how many icecreams are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are five ducks\n",
      "Decoded sentence: how many ducks are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are four mangos\n",
      "Decoded sentence: how many mangos are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are six balls\n",
      "Decoded sentence: how many balls are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are one car\n",
      "Decoded sentence: how many cars are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are four carrots\n",
      "Decoded sentence: how many carrots are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are two lollipops\n",
      "Decoded sentence: how many lollipops are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are six lamps\n",
      "Decoded sentence: how many lamps are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are three sharks\n",
      "Decoded sentence: how many sharks are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are seven snakes\n",
      "Decoded sentence: how many snakes are there? seven\n",
      "\n",
      "-\n",
      "Input sentence: there are one zirafah\n",
      "Decoded sentence: how many zirafahs are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two teddys\n",
      "Decoded sentence: how many teddys are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are six ducks\n",
      "Decoded sentence: how many ducks are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are two eapots\n",
      "Decoded sentence: how many eapots are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are one pencil\n",
      "Decoded sentence: how many pencils are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two cupcakes\n",
      "Decoded sentence: how many cupcakes are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are two pumpkins\n",
      "Decoded sentence: how many pumpkins are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are one flower\n",
      "Decoded sentence: how many flowers are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are five snakes\n",
      "Decoded sentence: how many snakes are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are two caps\n",
      "Decoded sentence: how many caps are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are one ladybug\n",
      "Decoded sentence: how many ladybugs are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two lamps\n",
      "Decoded sentence: how many lamps are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are six octopuss\n",
      "Decoded sentence: how many octopuss are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are two elephants\n",
      "Decoded sentence: how many elephants are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are one mango\n",
      "Decoded sentence: how many mangos are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are six suns\n",
      "Decoded sentence: how many suns are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are one orange\n",
      "Decoded sentence: how many oranges are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are five carrots\n",
      "Decoded sentence: how many carrots are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are four lamps\n",
      "Decoded sentence: how many lamps are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are one orange\n",
      "Decoded sentence: how many oranges are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are one octopus\n",
      "Decoded sentence: how many octopuss are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are five cows\n",
      "Decoded sentence: how many cows are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are four carrots\n",
      "Decoded sentence: how many carrots are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are five sharks\n",
      "Decoded sentence: how many sharks are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are one teddy\n",
      "Decoded sentence: how many teddys are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two eapots\n",
      "Decoded sentence: how many eapots are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are three butterflys\n",
      "Decoded sentence: how many butterflys are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are two eapots\n",
      "Decoded sentence: how many eapots are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are five rabbits\n",
      "Decoded sentence: how many rabbits are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are four balls\n",
      "Decoded sentence: how many balls are there? four\n",
      "\n",
      "-\n",
      "Input sentence: there are one icecream\n",
      "Decoded sentence: how many icecreams are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are six birds\n",
      "Decoded sentence: how many birds are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are two suns\n",
      "Decoded sentence: how many suns are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are seven teddys\n",
      "Decoded sentence: how many teddys are there? seven\n",
      "\n",
      "-\n",
      "Input sentence: there are one pumpkin\n",
      "Decoded sentence: how many pumpkins are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are five owls\n",
      "Decoded sentence: how many owls are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are five balls\n",
      "Decoded sentence: how many balls are there? five\n",
      "\n",
      "-\n",
      "Input sentence: there are two caterpillars\n",
      "Decoded sentence: how many caterpillars are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are five rabbits\n",
      "Decoded sentence: how many rabbits are there? five\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: there are two lollipops\n",
      "Decoded sentence: how many lollipops are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are one spider\n",
      "Decoded sentence: how many spiders are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are three bananas\n",
      "Decoded sentence: how many bananas are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are six mangos\n",
      "Decoded sentence: how many mangos are there? six\n",
      "\n",
      "-\n",
      "Input sentence: there are three pencils\n",
      "Decoded sentence: how many pencils are there? three\n",
      "\n",
      "-\n",
      "Input sentence: there are one lollipop\n",
      "Decoded sentence: how many lollipops are there? one\n",
      "\n",
      "-\n",
      "Input sentence: there are two oranges\n",
      "Decoded sentence: how many oranges are there? two\n",
      "\n",
      "-\n",
      "Input sentence: there are five balls\n",
      "Decoded sentence: how many balls are there? five\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
